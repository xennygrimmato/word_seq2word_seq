{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wseq2wseq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xennygrimmato/word_seq2word_seq/blob/master/src/wseq2wseq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "3yj8zc2qj5bs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "69290f72-f9cd-4b48-d099-2de76596de01"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (5.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a_Qp5uOjjb5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pprint\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jg9qN24MlJoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0e0a6761-233b-48b3-f152-573174bc9643"
      },
      "cell_type": "code",
      "source": [
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.3.104.250:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 12393550267468129861),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 15823551721832383097),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9726682145300103439),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 11311451706919437127),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13544090267815120828),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 4672089549160105293),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12628294551219372631),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 7123737919336338482),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 17342419875027621641),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 5180275439760608691),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 7393808385221385398)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bALBanxalD2x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = 'fra.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7eoQN7P-lPtP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "\n",
        "# File format: <eng-word>\\t<french-word>\\n\n",
        "# Get all lines from file\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "input_sequences = []\n",
        "output_sequences = []\n",
        "# Go through number of samples you want to train on\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    input_token_list = input_text.split()\n",
        "    output_token_list = target_text.split()\n",
        "    input_sequences.append(input_token_list)\n",
        "    output_sequences.append(output_token_list)\n",
        "    for word in input_token_list:\n",
        "        if word not in input_words:\n",
        "            input_words.add(word)\n",
        "    for word in output_token_list:\n",
        "        if word not in target_words:\n",
        "            target_words.add(word)\n",
        "\n",
        "input_words = sorted(list(input_words))\n",
        "target_words = sorted(list(target_words))\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "max_encoder_seq_length = max([len(seq) for seq in input_sequences])\n",
        "max_decoder_seq_length = max([len(seq) for seq in output_sequences])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "\n",
        "input_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict(\n",
        "    [(word, i) for i, word in enumerate(target_words)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VO1ODG-glSkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare data to be ingested by the model\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_sequences, output_sequences) in enumerate(zip(input_sequences, output_sequences)):\n",
        "    for t, word in enumerate(input_sequences):\n",
        "        encoder_input_data[i, t, input_token_index[word]] = 1.\n",
        "    for t, word in enumerate(output_sequences):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[word]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "448MMzuglegA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "59ySSKpfliB2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxJ7BORdlfyn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metadata = {\n",
        "    'num_encoder_tokens': num_encoder_tokens,\n",
        "    'num_decoder_tokens': num_decoder_tokens\n",
        "}\n",
        "\n",
        "json.dump(metadata, open('metadata.json', 'w'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}